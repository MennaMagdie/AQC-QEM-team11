{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-k2dOueZBWDZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "wokf1d9jBh3r",
        "outputId": "3e121253-46b0-40c3-9db9-57b46082f2e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   n_qubits  n_gates  depth  ideal_0  ideal_1  ideal_2  ideal_3  ideal_4  \\\n",
              "0         3        8      3   1.0000      0.0      0.0   0.0000      0.0   \n",
              "1         4       16      7   0.4977      0.0      0.0   0.0000      0.0   \n",
              "2         4       10      4   1.0000      0.0      0.0   0.0000      0.0   \n",
              "3         4        7      2   0.4991      0.0      0.0   0.0000      0.0   \n",
              "4         2        4      2   0.0112      0.0      0.0   0.9888      0.0   \n",
              "\n",
              "   ideal_5  ideal_6  ...  high_6  high_7  high_8  high_9  high_10  high_11  \\\n",
              "0      0.0      0.0  ...  0.1225  0.0607  0.0000  0.0000   0.0000   0.0000   \n",
              "1      0.0      0.0  ...  0.0677  0.0388  0.0765  0.0502   0.0664   0.0431   \n",
              "2      0.0      0.0  ...  0.0448  0.0351  0.0833  0.0539   0.0442   0.0396   \n",
              "3      0.0      0.0  ...  0.0630  0.0382  0.0744  0.0458   0.0499   0.0299   \n",
              "4      0.0      0.0  ...  0.0000  0.0000  0.0000  0.0000   0.0000   0.0000   \n",
              "\n",
              "   high_12  high_13  high_14  high_15  \n",
              "0   0.0000   0.0000   0.0000   0.0000  \n",
              "1   0.0497   0.0355   0.0405   0.0303  \n",
              "2   0.0635   0.0423   0.0334   0.0323  \n",
              "3   0.0667   0.0421   0.0385   0.0261  \n",
              "4   0.0000   0.0000   0.0000   0.0000  \n",
              "\n",
              "[5 rows x 67 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c4c2ecf3-6b03-4c93-b827-78607d85e00d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_qubits</th>\n",
              "      <th>n_gates</th>\n",
              "      <th>depth</th>\n",
              "      <th>ideal_0</th>\n",
              "      <th>ideal_1</th>\n",
              "      <th>ideal_2</th>\n",
              "      <th>ideal_3</th>\n",
              "      <th>ideal_4</th>\n",
              "      <th>ideal_5</th>\n",
              "      <th>ideal_6</th>\n",
              "      <th>...</th>\n",
              "      <th>high_6</th>\n",
              "      <th>high_7</th>\n",
              "      <th>high_8</th>\n",
              "      <th>high_9</th>\n",
              "      <th>high_10</th>\n",
              "      <th>high_11</th>\n",
              "      <th>high_12</th>\n",
              "      <th>high_13</th>\n",
              "      <th>high_14</th>\n",
              "      <th>high_15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1225</td>\n",
              "      <td>0.0607</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>7</td>\n",
              "      <td>0.4977</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0677</td>\n",
              "      <td>0.0388</td>\n",
              "      <td>0.0765</td>\n",
              "      <td>0.0502</td>\n",
              "      <td>0.0664</td>\n",
              "      <td>0.0431</td>\n",
              "      <td>0.0497</td>\n",
              "      <td>0.0355</td>\n",
              "      <td>0.0405</td>\n",
              "      <td>0.0303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0448</td>\n",
              "      <td>0.0351</td>\n",
              "      <td>0.0833</td>\n",
              "      <td>0.0539</td>\n",
              "      <td>0.0442</td>\n",
              "      <td>0.0396</td>\n",
              "      <td>0.0635</td>\n",
              "      <td>0.0423</td>\n",
              "      <td>0.0334</td>\n",
              "      <td>0.0323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>0.4991</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0630</td>\n",
              "      <td>0.0382</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0458</td>\n",
              "      <td>0.0499</td>\n",
              "      <td>0.0299</td>\n",
              "      <td>0.0667</td>\n",
              "      <td>0.0421</td>\n",
              "      <td>0.0385</td>\n",
              "      <td>0.0261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9888</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 67 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4c2ecf3-6b03-4c93-b827-78607d85e00d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c4c2ecf3-6b03-4c93-b827-78607d85e00d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c4c2ecf3-6b03-4c93-b827-78607d85e00d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d03bd41d-eccc-415a-b97a-95f4074ce117\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d03bd41d-eccc-415a-b97a-95f4074ce117')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d03bd41d-eccc-415a-b97a-95f4074ce117 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Importing derived data\n",
        "\n",
        "df = pd.read_csv('quantum_dataset.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "TBBPXFVxB-sD",
        "outputId": "9d9741df-b85f-4279-9960-dd96c7a010f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           n_qubits       n_gates         depth       ideal_0       ideal_1  \\\n",
              "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
              "mean       2.998000     10.847600      5.000100      0.364507      0.134384   \n",
              "std        0.817106      4.113401      2.002873      0.368379      0.222738   \n",
              "min        2.000000      4.000000      2.000000      0.000000      0.000000   \n",
              "25%        2.000000      8.000000      3.000000      0.021200      0.000000   \n",
              "50%        3.000000     10.000000      5.000000      0.245750      0.000400   \n",
              "75%        4.000000     13.000000      7.000000      0.587300      0.207825   \n",
              "max        4.000000     34.000000      8.000000      1.000000      1.000000   \n",
              "\n",
              "            ideal_2       ideal_3       ideal_4       ideal_5       ideal_6  \\\n",
              "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
              "mean       0.137997      0.104714      0.065676      0.036966      0.036518   \n",
              "std        0.228096      0.193510      0.174708      0.117023      0.117229   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "50%        0.000400      0.000000      0.000000      0.000000      0.000000   \n",
              "75%        0.216175      0.129225      0.003600      0.000000      0.000000   \n",
              "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
              "\n",
              "       ...        high_6        high_7        high_8        high_9  \\\n",
              "count  ...  10000.000000  10000.000000  10000.000000  10000.000000   \n",
              "mean   ...      0.052495      0.038044      0.027997      0.019178   \n",
              "std    ...      0.044462      0.033047      0.040984      0.028289   \n",
              "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
              "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
              "50%    ...      0.055000      0.038800      0.000000      0.000000   \n",
              "75%    ...      0.088500      0.066300      0.074100      0.049300   \n",
              "max    ...      0.310000      0.260000      0.241700      0.214000   \n",
              "\n",
              "            high_10       high_11       high_12       high_13       high_14  \\\n",
              "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
              "mean       0.019131      0.013420      0.018936      0.013303      0.013298   \n",
              "std        0.028257      0.019966      0.027870      0.019720      0.019771   \n",
              "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
              "75%        0.049300      0.035000      0.049100      0.034800      0.034700   \n",
              "max        0.226500      0.164100      0.258000      0.162000      0.194500   \n",
              "\n",
              "            high_15  \n",
              "count  10000.000000  \n",
              "mean       0.009502  \n",
              "std        0.014180  \n",
              "min        0.000000  \n",
              "25%        0.000000  \n",
              "50%        0.000000  \n",
              "75%        0.024300  \n",
              "max        0.087000  \n",
              "\n",
              "[8 rows x 67 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9966f714-fd18-4109-b718-fe1d1f53005d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_qubits</th>\n",
              "      <th>n_gates</th>\n",
              "      <th>depth</th>\n",
              "      <th>ideal_0</th>\n",
              "      <th>ideal_1</th>\n",
              "      <th>ideal_2</th>\n",
              "      <th>ideal_3</th>\n",
              "      <th>ideal_4</th>\n",
              "      <th>ideal_5</th>\n",
              "      <th>ideal_6</th>\n",
              "      <th>...</th>\n",
              "      <th>high_6</th>\n",
              "      <th>high_7</th>\n",
              "      <th>high_8</th>\n",
              "      <th>high_9</th>\n",
              "      <th>high_10</th>\n",
              "      <th>high_11</th>\n",
              "      <th>high_12</th>\n",
              "      <th>high_13</th>\n",
              "      <th>high_14</th>\n",
              "      <th>high_15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.998000</td>\n",
              "      <td>10.847600</td>\n",
              "      <td>5.000100</td>\n",
              "      <td>0.364507</td>\n",
              "      <td>0.134384</td>\n",
              "      <td>0.137997</td>\n",
              "      <td>0.104714</td>\n",
              "      <td>0.065676</td>\n",
              "      <td>0.036966</td>\n",
              "      <td>0.036518</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052495</td>\n",
              "      <td>0.038044</td>\n",
              "      <td>0.027997</td>\n",
              "      <td>0.019178</td>\n",
              "      <td>0.019131</td>\n",
              "      <td>0.013420</td>\n",
              "      <td>0.018936</td>\n",
              "      <td>0.013303</td>\n",
              "      <td>0.013298</td>\n",
              "      <td>0.009502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.817106</td>\n",
              "      <td>4.113401</td>\n",
              "      <td>2.002873</td>\n",
              "      <td>0.368379</td>\n",
              "      <td>0.222738</td>\n",
              "      <td>0.228096</td>\n",
              "      <td>0.193510</td>\n",
              "      <td>0.174708</td>\n",
              "      <td>0.117023</td>\n",
              "      <td>0.117229</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044462</td>\n",
              "      <td>0.033047</td>\n",
              "      <td>0.040984</td>\n",
              "      <td>0.028289</td>\n",
              "      <td>0.028257</td>\n",
              "      <td>0.019966</td>\n",
              "      <td>0.027870</td>\n",
              "      <td>0.019720</td>\n",
              "      <td>0.019771</td>\n",
              "      <td>0.014180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.021200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.245750</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055000</td>\n",
              "      <td>0.038800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.587300</td>\n",
              "      <td>0.207825</td>\n",
              "      <td>0.216175</td>\n",
              "      <td>0.129225</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088500</td>\n",
              "      <td>0.066300</td>\n",
              "      <td>0.074100</td>\n",
              "      <td>0.049300</td>\n",
              "      <td>0.049300</td>\n",
              "      <td>0.035000</td>\n",
              "      <td>0.049100</td>\n",
              "      <td>0.034800</td>\n",
              "      <td>0.034700</td>\n",
              "      <td>0.024300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.241700</td>\n",
              "      <td>0.214000</td>\n",
              "      <td>0.226500</td>\n",
              "      <td>0.164100</td>\n",
              "      <td>0.258000</td>\n",
              "      <td>0.162000</td>\n",
              "      <td>0.194500</td>\n",
              "      <td>0.087000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 67 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9966f714-fd18-4109-b718-fe1d1f53005d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9966f714-fd18-4109-b718-fe1d1f53005d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9966f714-fd18-4109-b718-fe1d1f53005d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f670a3d5-a1db-4583-be8e-4874bfa5ddf5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f670a3d5-a1db-4583-be8e-4874bfa5ddf5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f670a3d5-a1db-4583-be8e-4874bfa5ddf5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Description of the data\n",
        "\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "UQI7KgW8CE17",
        "outputId": "30ab68ba-df51-46f7-cc3f-ebcf6cc9a960"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "n_qubits    0\n",
              "n_gates     0\n",
              "depth       0\n",
              "ideal_0     0\n",
              "ideal_1     0\n",
              "           ..\n",
              "high_11     0\n",
              "high_12     0\n",
              "high_13     0\n",
              "high_14     0\n",
              "high_15     0\n",
              "Length: 67, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>n_qubits</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n_gates</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>depth</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ideal_0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ideal_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>high_11</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>high_12</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>high_13</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>high_14</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>high_15</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Checking for nulls\n",
        "\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ef55cc2",
        "outputId": "89c56943-df96-4928-b39a-2a57a16ab247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['n_qubits', 'n_gates', 'depth', 'ideal_0', 'ideal_1', 'ideal_2',\n",
            "       'ideal_3', 'ideal_4', 'ideal_5', 'ideal_6', 'ideal_7', 'ideal_8',\n",
            "       'ideal_9', 'ideal_10', 'ideal_11', 'ideal_12', 'ideal_13', 'ideal_14',\n",
            "       'ideal_15', 'low_0', 'low_1', 'low_2', 'low_3', 'low_4', 'low_5',\n",
            "       'low_6', 'low_7', 'low_8', 'low_9', 'low_10', 'low_11', 'low_12',\n",
            "       'low_13', 'low_14', 'low_15', 'moderate_0', 'moderate_1', 'moderate_2',\n",
            "       'moderate_3', 'moderate_4', 'moderate_5', 'moderate_6', 'moderate_7',\n",
            "       'moderate_8', 'moderate_9', 'moderate_10', 'moderate_11', 'moderate_12',\n",
            "       'moderate_13', 'moderate_14', 'moderate_15', 'high_0', 'high_1',\n",
            "       'high_2', 'high_3', 'high_4', 'high_5', 'high_6', 'high_7', 'high_8',\n",
            "       'high_9', 'high_10', 'high_11', 'high_12', 'high_13', 'high_14',\n",
            "       'high_15'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t9ZkN-4C8YHd"
      },
      "outputs": [],
      "source": [
        "dfs = {}\n",
        "\n",
        "for i in range(16):\n",
        "    dfs[f\"df_{i}\"] = df[\n",
        "        [\n",
        "            \"n_qubits\",\n",
        "            \"n_gates\",\n",
        "            \"depth\",\n",
        "            f\"ideal_{i}\",\n",
        "            f\"low_{i}\",\n",
        "            f\"moderate_{i}\",\n",
        "            f\"high_{i}\",\n",
        "        ]\n",
        "    ].copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create individual dataframes dynamically\n",
        "\n",
        "for i in range(16):\n",
        "    globals()[f\"df_{i}\"] = dfs[f\"df_{i}\"]\n"
      ],
      "metadata": {
        "id": "-KgT3JyFC5_b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "for i in range(16):\n",
        "    # Extract the dataframe\n",
        "    df = globals()[f\"df_{i}\"]\n",
        "\n",
        "    # Separate features (X) and target (y)\n",
        "    X = df.drop(columns=[\"n_qubits\", \"n_gates\", \"depth\", f\"ideal_{i}\"])\n",
        "    y = df[f\"ideal_{i}\"]\n",
        "\n",
        "    # Split the data into train and test sets (80% train, 20% test)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Feature scaling (normalize data for deep learning)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Build a deep learning model (simple example)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1))  # Single output node for regression (ideal values)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss = model.evaluate(X_test, y_test)\n",
        "    print(f\"Model {i} Test Loss: {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV9s0zN_CCZs",
        "outputId": "4d37417a-02a7-4104-b48c-0156ea00ccd7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0865 - val_loss: 0.0395\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0403 - val_loss: 0.0368\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0416 - val_loss: 0.0368\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0370 - val_loss: 0.0353\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0383 - val_loss: 0.0346\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0377 - val_loss: 0.0349\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0393 - val_loss: 0.0337\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0382 - val_loss: 0.0333\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0375 - val_loss: 0.0341\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0376 - val_loss: 0.0341\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0364 - val_loss: 0.0344\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0364 - val_loss: 0.0335\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0343\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0352 - val_loss: 0.0339\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0355 - val_loss: 0.0336\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0370 - val_loss: 0.0335\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0353 - val_loss: 0.0326\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0367 - val_loss: 0.0334\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0355 - val_loss: 0.0323\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0362 - val_loss: 0.0322\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0357 - val_loss: 0.0317\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0386 - val_loss: 0.0347\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0328\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0348 - val_loss: 0.0320\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0344 - val_loss: 0.0320\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0369 - val_loss: 0.0320\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0355 - val_loss: 0.0329\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0356 - val_loss: 0.0317\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0332 - val_loss: 0.0313\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0359 - val_loss: 0.0318\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0344 - val_loss: 0.0334\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0357 - val_loss: 0.0321\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0355 - val_loss: 0.0321\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0366 - val_loss: 0.0326\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0356 - val_loss: 0.0329\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0362 - val_loss: 0.0315\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0348 - val_loss: 0.0322\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0338 - val_loss: 0.0356\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0351 - val_loss: 0.0324\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0340 - val_loss: 0.0321\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0347 - val_loss: 0.0318\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0357 - val_loss: 0.0315\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0346 - val_loss: 0.0313\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0337 - val_loss: 0.0319\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0339 - val_loss: 0.0313\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0336 - val_loss: 0.0315\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0349 - val_loss: 0.0311\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0348 - val_loss: 0.0317\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0342 - val_loss: 0.0308\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0322\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0314\n",
            "Model 0 Test Loss: 0.03223283588886261\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0368 - val_loss: 0.0141\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - val_loss: 0.0139\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - val_loss: 0.0141\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0118 - val_loss: 0.0130\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0121 - val_loss: 0.0134\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0132\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0100 - val_loss: 0.0131\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0129\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0121 - val_loss: 0.0130\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0135\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0129\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0127\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0133\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0109 - val_loss: 0.0135\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0107 - val_loss: 0.0127\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0116 - val_loss: 0.0132\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0120 - val_loss: 0.0127\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0116 - val_loss: 0.0127\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 0.0130\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0121 - val_loss: 0.0128\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0133\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0140\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0128\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0128\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0126\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0129\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - val_loss: 0.0126\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0105 - val_loss: 0.0128\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0129\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - val_loss: 0.0127\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0127\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0128\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0130\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0118 - val_loss: 0.0127\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0113 - val_loss: 0.0128\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0110 - val_loss: 0.0128\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0129\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - val_loss: 0.0129\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0127\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0129\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0128\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - val_loss: 0.0126\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0106 - val_loss: 0.0130\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0109 - val_loss: 0.0127\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0107 - val_loss: 0.0127\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0120 - val_loss: 0.0128\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0106 - val_loss: 0.0132\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0129\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0129\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0125\n",
            "Model 1 Test Loss: 0.012887869030237198\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0443 - val_loss: 0.0123\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0134 - val_loss: 0.0123\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0129 - val_loss: 0.0119\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0116 - val_loss: 0.0121\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0140 - val_loss: 0.0119\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0134 - val_loss: 0.0121\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0117\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0129 - val_loss: 0.0117\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0122 - val_loss: 0.0120\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0121 - val_loss: 0.0118\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0123 - val_loss: 0.0122\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0125 - val_loss: 0.0124\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0129 - val_loss: 0.0130\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0123 - val_loss: 0.0117\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0134 - val_loss: 0.0118\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0130 - val_loss: 0.0121\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0123 - val_loss: 0.0120\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0131 - val_loss: 0.0116\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0128 - val_loss: 0.0119\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0129 - val_loss: 0.0119\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0121 - val_loss: 0.0115\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0120 - val_loss: 0.0119\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0119\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0128 - val_loss: 0.0121\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0126 - val_loss: 0.0124\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - val_loss: 0.0116\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0125 - val_loss: 0.0116\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - val_loss: 0.0135\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0127 - val_loss: 0.0116\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0130 - val_loss: 0.0116\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0117\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0118 - val_loss: 0.0115\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - val_loss: 0.0117\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0116\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0129 - val_loss: 0.0117\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0109 - val_loss: 0.0117\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0131 - val_loss: 0.0117\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0120 - val_loss: 0.0116\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0123 - val_loss: 0.0115\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0127 - val_loss: 0.0116\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0118 - val_loss: 0.0117\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0116\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0130 - val_loss: 0.0116\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0127 - val_loss: 0.0116\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0117\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0128 - val_loss: 0.0118\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - val_loss: 0.0117\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0117\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0103\n",
            "Model 2 Test Loss: 0.011663415469229221\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0194 - val_loss: 0.0052\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0052 - val_loss: 0.0050\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0051\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0050\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0050\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0045 - val_loss: 0.0051\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0050 - val_loss: 0.0048\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0042 - val_loss: 0.0051\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0045 - val_loss: 0.0048\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0042 - val_loss: 0.0048\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0048\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0052 - val_loss: 0.0049\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0048\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0048\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0051\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0058\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0047 - val_loss: 0.0048\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0040 - val_loss: 0.0048\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0047\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0048\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0053 - val_loss: 0.0047\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0048 - val_loss: 0.0049\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0052\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0045 - val_loss: 0.0047\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0044 - val_loss: 0.0050\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0048\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0045 - val_loss: 0.0048\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0040 - val_loss: 0.0049\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0051\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0042 - val_loss: 0.0050\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0049\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0048\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0047\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0051 - val_loss: 0.0049\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0049\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0049\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0044 - val_loss: 0.0053\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0048\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0048\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0042 - val_loss: 0.0048\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0047\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0045 - val_loss: 0.0047\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0048\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0049\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0040 - val_loss: 0.0050\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0048\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0045 - val_loss: 0.0049\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0048\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0054\n",
            "Model 3 Test Loss: 0.004750586114823818\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0238 - val_loss: 0.0133\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0123\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0122\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0132 - val_loss: 0.0122\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0118 - val_loss: 0.0121\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0123 - val_loss: 0.0139\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - val_loss: 0.0117\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0119\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0120 - val_loss: 0.0121\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0115 - val_loss: 0.0120\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0109 - val_loss: 0.0119\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0109 - val_loss: 0.0118\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0120\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0118\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0103 - val_loss: 0.0117\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0118\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0118 - val_loss: 0.0117\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0109 - val_loss: 0.0121\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0110 - val_loss: 0.0117\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - val_loss: 0.0118\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0116 - val_loss: 0.0118\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0122\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0123 - val_loss: 0.0120\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0116 - val_loss: 0.0122\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0108 - val_loss: 0.0124\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0116 - val_loss: 0.0126\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0111 - val_loss: 0.0117\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0104 - val_loss: 0.0127\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0102 - val_loss: 0.0117\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0117\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - val_loss: 0.0119\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - val_loss: 0.0119\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0121\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0118\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - val_loss: 0.0118\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0120\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0117\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - val_loss: 0.0117\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0107 - val_loss: 0.0118\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0101 - val_loss: 0.0119\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0104 - val_loss: 0.0118\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0119\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0098 - val_loss: 0.0117\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0113 - val_loss: 0.0117\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0109 - val_loss: 0.0118\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0098 - val_loss: 0.0117\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0118\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0109 - val_loss: 0.0120\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0140\n",
            "Model 4 Test Loss: 0.01197531446814537\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0093 - val_loss: 0.0056\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0054 - val_loss: 0.0054\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0045 - val_loss: 0.0053\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0055\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0056\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0052 - val_loss: 0.0053\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0051\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0051 - val_loss: 0.0054\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0051\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0051\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0051\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0051 - val_loss: 0.0051\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0044 - val_loss: 0.0051\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0053\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0051\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0053\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0050\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0051\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0051\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0051\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0044 - val_loss: 0.0054\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0050\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0051 - val_loss: 0.0050\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0048 - val_loss: 0.0052\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0051\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0051 - val_loss: 0.0052\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0054 - val_loss: 0.0050\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0052 - val_loss: 0.0050\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0044 - val_loss: 0.0051\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0045 - val_loss: 0.0053\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0051 - val_loss: 0.0052\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0045 - val_loss: 0.0051\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0058\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0050\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0051\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0053 - val_loss: 0.0051\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0045 - val_loss: 0.0050\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0049 - val_loss: 0.0050\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0051\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0052 - val_loss: 0.0052\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0051\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0051 - val_loss: 0.0051\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0045 - val_loss: 0.0050\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0045 - val_loss: 0.0051\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0044 - val_loss: 0.0051\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0049 - val_loss: 0.0050\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0051\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0051 - val_loss: 0.0050\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0050\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0053\n",
            "Model 5 Test Loss: 0.004974735900759697\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0193 - val_loss: 0.0053\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0056 - val_loss: 0.0047\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0054 - val_loss: 0.0046\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0052 - val_loss: 0.0044\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0044\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0054 - val_loss: 0.0044\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0056 - val_loss: 0.0044\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0054 - val_loss: 0.0044\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0051 - val_loss: 0.0044\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0045\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0045 - val_loss: 0.0050\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0057 - val_loss: 0.0044\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0052 - val_loss: 0.0044\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0052 - val_loss: 0.0049\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0057 - val_loss: 0.0043\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0044\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0044\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0045\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0044\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0064 - val_loss: 0.0044\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0054 - val_loss: 0.0048\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0044\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0052 - val_loss: 0.0045\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0052 - val_loss: 0.0043\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.0045\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0048 - val_loss: 0.0043\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0052 - val_loss: 0.0043\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0046\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.0044\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0057 - val_loss: 0.0045\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0043\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0057 - val_loss: 0.0044\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0047\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0056 - val_loss: 0.0045\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0043\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0044 - val_loss: 0.0045\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0046 - val_loss: 0.0044\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - val_loss: 0.0045\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0045\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0053 - val_loss: 0.0043\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0050\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0044\n",
            "Model 6 Test Loss: 0.005044993478804827\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0067 - val_loss: 0.0065\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0046\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033 - val_loss: 0.0044\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034 - val_loss: 0.0046\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0044\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033 - val_loss: 0.0043\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033 - val_loss: 0.0057\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034 - val_loss: 0.0046\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0044\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0032 - val_loss: 0.0044\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0043\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0046\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0043\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0030 - val_loss: 0.0045\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0043\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0030 - val_loss: 0.0047\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0028 - val_loss: 0.0044\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0027 - val_loss: 0.0043\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0051\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0043\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0043\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0043\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0043\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0043\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0042\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0043\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0043\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0047\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0045\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0043\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0042\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0044\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0044\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0027 - val_loss: 0.0043\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0027 - val_loss: 0.0043\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0026 - val_loss: 0.0042\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0032 - val_loss: 0.0042\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0043\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0043\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033 - val_loss: 0.0043\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0044\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0042\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0044\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0043\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0043\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0043\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0044\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0042\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0044\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0044\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0049\n",
            "Model 7 Test Loss: 0.004428667947649956\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0116 - val_loss: 0.0079\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0069 - val_loss: 0.0077\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0072 - val_loss: 0.0083\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0074 - val_loss: 0.0076\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0064 - val_loss: 0.0075\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0074 - val_loss: 0.0095\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0078 - val_loss: 0.0082\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0068 - val_loss: 0.0076\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - val_loss: 0.0077\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0066 - val_loss: 0.0076\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0072 - val_loss: 0.0075\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - val_loss: 0.0085\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0070 - val_loss: 0.0075\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0062 - val_loss: 0.0078\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0068 - val_loss: 0.0076\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0077 - val_loss: 0.0081\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0067 - val_loss: 0.0076\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - val_loss: 0.0076\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0067 - val_loss: 0.0077\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0073 - val_loss: 0.0076\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0072 - val_loss: 0.0076\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0066 - val_loss: 0.0076\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0072 - val_loss: 0.0079\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0076 - val_loss: 0.0076\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0067 - val_loss: 0.0080\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - val_loss: 0.0081\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0069 - val_loss: 0.0080\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - val_loss: 0.0077\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0071 - val_loss: 0.0075\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0063 - val_loss: 0.0076\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0059 - val_loss: 0.0074\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0074 - val_loss: 0.0088\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0083 - val_loss: 0.0076\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0070 - val_loss: 0.0075\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0063 - val_loss: 0.0076\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0074 - val_loss: 0.0077\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0059 - val_loss: 0.0076\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0065 - val_loss: 0.0075\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0061 - val_loss: 0.0079\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0061 - val_loss: 0.0075\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0072 - val_loss: 0.0077\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0066 - val_loss: 0.0075\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0062 - val_loss: 0.0077\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0061 - val_loss: 0.0076\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - val_loss: 0.0075\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0067 - val_loss: 0.0075\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0062 - val_loss: 0.0076\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0067 - val_loss: 0.0079\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0062 - val_loss: 0.0081\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0066 - val_loss: 0.0076\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0080\n",
            "Model 8 Test Loss: 0.0076326713897287846\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0093 - val_loss: 0.0026\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0026\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0025\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0026\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0030\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0023\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033 - val_loss: 0.0023\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0024\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0024\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0025\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0023\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0023\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0025\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0023\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0024 - val_loss: 0.0025\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033 - val_loss: 0.0023\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0023\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0023\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0023\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021 - val_loss: 0.0029\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0024\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0023 - val_loss: 0.0024\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0023\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0024\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026\n",
            "Model 9 Test Loss: 0.0023934643249958754\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0061 - val_loss: 0.0022\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0034 - val_loss: 0.0021\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0022\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0021\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0024\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0020\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0020\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0021\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0022\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0023\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0037 - val_loss: 0.0021\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0029 - val_loss: 0.0021\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0020\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0022\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034 - val_loss: 0.0021\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0021\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0021\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0021\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0020\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0026\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0031 - val_loss: 0.0021\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0032 - val_loss: 0.0023\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0021\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0032 - val_loss: 0.0021\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0021\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034 - val_loss: 0.0021\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0035 - val_loss: 0.0022\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0022\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0020\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0025 - val_loss: 0.0022\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0028 - val_loss: 0.0021\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0031 - val_loss: 0.0021\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033 - val_loss: 0.0021\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0028\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0022\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0023\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0021\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0024\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0024\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0021\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0021\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0032 - val_loss: 0.0020\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0022\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0021\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0021\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0022\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0032 - val_loss: 0.0021\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - val_loss: 0.0021\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0028 - val_loss: 0.0021\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0020\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0022\n",
            "Model 10 Test Loss: 0.00200925231911242\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0046 - val_loss: 0.0017\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0014\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0019\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0016\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 0.0014\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0013\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.5307e-04 - val_loss: 0.0014\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0013\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.3314e-04\n",
            "Model 11 Test Loss: 0.0013723727315664291\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0238 - val_loss: 0.0023\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0020 - val_loss: 0.0027\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0023 - val_loss: 0.0022\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021 - val_loss: 0.0032\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0020\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0030\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0030 - val_loss: 0.0023\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0023\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0020\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - val_loss: 0.0022\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0023 - val_loss: 0.0038\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0024 - val_loss: 0.0029\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0025\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - val_loss: 0.0020\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0022\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - val_loss: 0.0020\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0021\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0025 - val_loss: 0.0024\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021 - val_loss: 0.0021\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0024\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0020 - val_loss: 0.0020\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 0.0026\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0020\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0020\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0024\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024 - val_loss: 0.0021\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025\n",
            "Model 12 Test Loss: 0.0019726320169866085\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0023 - val_loss: 0.0014\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.6205e-04 - val_loss: 0.0013\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.4972e-04 - val_loss: 0.0013\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.8530e-04 - val_loss: 0.0014\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.9754e-04 - val_loss: 0.0013\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.6221e-04 - val_loss: 0.0013\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.4537e-04 - val_loss: 0.0012\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.4043e-04 - val_loss: 0.0013\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.3645e-04 - val_loss: 0.0014\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.3105e-04 - val_loss: 0.0018\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.0717e-04 - val_loss: 0.0013\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.9399e-04 - val_loss: 0.0012\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.3139e-04 - val_loss: 0.0012\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.2047e-04 - val_loss: 0.0012\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.8538e-04 - val_loss: 0.0013\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 8.0361e-04 - val_loss: 0.0012\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.6485e-04 - val_loss: 0.0011\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.0161e-04 - val_loss: 0.0012\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.1460e-04 - val_loss: 0.0012\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0012\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6148e-04 - val_loss: 0.0012\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.7792e-04 - val_loss: 0.0012\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.7716e-04 - val_loss: 0.0012\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6872e-04 - val_loss: 0.0012\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.1452e-04 - val_loss: 0.0013\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.6854e-04 - val_loss: 0.0014\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6917e-04 - val_loss: 0.0013\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.8747e-04 - val_loss: 0.0013\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.8500e-04 - val_loss: 0.0013\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.8368e-04 - val_loss: 0.0013\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.8611e-04 - val_loss: 0.0013\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 8.1220e-04 - val_loss: 0.0012\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.0098e-04 - val_loss: 0.0013\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.5424e-04 - val_loss: 0.0014\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.4724e-04 - val_loss: 0.0012\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.2048e-04 - val_loss: 0.0013\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.5086e-04 - val_loss: 0.0013\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.9069e-04 - val_loss: 0.0014\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0012\n",
            "Model 13 Test Loss: 0.0013985228724777699\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0020 - val_loss: 0.0016\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0023\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 0.0016\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.1554e-04 - val_loss: 0.0015\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.4544e-04 - val_loss: 0.0015\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0016\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.5880e-04 - val_loss: 0.0015\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 0.0016\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.3948e-04 - val_loss: 0.0016\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.5787e-04 - val_loss: 0.0016\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 0.0015\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.2090e-04 - val_loss: 0.0015\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.6978e-04 - val_loss: 0.0015\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.8207e-04 - val_loss: 0.0015\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0016\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.5712e-04 - val_loss: 0.0015\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.2183e-04 - val_loss: 0.0015\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.5613e-04 - val_loss: 0.0015\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 9.6055e-04 - val_loss: 0.0015\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.7622e-04 - val_loss: 0.0015\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 9.5168e-04 - val_loss: 0.0015\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.7722e-04 - val_loss: 0.0015\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.4514e-04 - val_loss: 0.0015\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
            "Model 14 Test Loss: 0.001487844274379313\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0058 - val_loss: 0.0011\n",
            "Epoch 2/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 3/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 9.2099e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 6/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0015 - val_loss: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 9.6635e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0012\n",
            "Epoch 9/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 0.0011\n",
            "Epoch 10/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
            "Epoch 11/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0014 - val_loss: 9.1055e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0011 - val_loss: 8.7946e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 9.2344e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 9.8777e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 9.1088e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 8.8416e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 19/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 9.6986e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 8.6528e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 9.2541e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 9.2947e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 8.8316e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 8.3990e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 9.7507e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0016 - val_loss: 8.9821e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 8.9773e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 8.4258e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 9.1059e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0013 - val_loss: 8.7980e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0013 - val_loss: 8.6031e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0012 - val_loss: 9.5172e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 8.4129e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 8.7780e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.4153e-04 - val_loss: 8.5949e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 8.3924e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 9.4206e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 39/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 9.5629e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 9.7760e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0010 - val_loss: 8.5313e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0012 - val_loss: 8.2335e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 9.4086e-04 - val_loss: 9.0125e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0011 - val_loss: 8.5849e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 8.2498e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0014 - val_loss: 8.2453e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0013 - val_loss: 8.4207e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0013 - val_loss: 8.1256e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0012 - val_loss: 8.9330e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0015 - val_loss: 9.2955e-04\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0011\n",
            "Model 15 Test Loss: 0.0009295454947277904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual vs Predicted Plot"
      ],
      "metadata": {
        "id": "6KlwZkfLVC5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')  # Ideal line y = x\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "t4h5x39sU1N_",
        "outputId": "8175cc4d-1313-4b26-93f0-a3f01fc8f684"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Mectrics"
      ],
      "metadata": {
        "id": "xzC1xIzkTwza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# Predict the target values using the trained model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate error metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)  # RMSE\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the error metrics\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f'Mean squared error (MSE): {mse}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SbPq13PCCIg",
        "outputId": "433aca53-5027-46fb-df99-636756c5fd8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Mean Absolute Error (MAE): 0.009608042478834837\n",
            "Mean squared error (MSE): 0.0009295455523168545\n",
            "Root Mean Squared Error (RMSE): 0.030488449490206196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming your model is already trained\n",
        "model_filename = f'model_{i}.pkl'\n",
        "\n",
        "# Save the model as a pickle file\n",
        "with open(model_filename, 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(f\"Model {i} saved as {model_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "wnipL1oiCB5c",
        "outputId": "1ea07e02-2a4a-4593-ccbd-4d39cf2c3ebb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'i' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2662536017.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming your model is already trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'model_{i}.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Save the model as a pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}